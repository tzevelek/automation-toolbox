{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd2ef6b-f4ea-45a6-b407-a6755e7bff1e",
   "metadata": {},
   "source": [
    "## Data Processing with SageMaker Processing and Apache Spark - Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfb474-616d-41f8-a46a-9f5d345f6be9",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63223927-0039-4aee-a9d6-8a9edfd25c45",
   "metadata": {},
   "source": [
    "This notebook has been tested on SageMaker Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eda7f5ef-b88e-4e00-9ede-621ac5ef36f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-477886989750\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "433f5edd-b6fd-4317-a208-6226e20461d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files=1000\n",
    "num_instances=10\n",
    "num_spark_instances=9\n",
    "num_spark_cores=9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf45b3-6502-4519-9733-051659612a7c",
   "metadata": {},
   "source": [
    "# 2. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "870dcf33-a2b2-40e3-9a99-62360869482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3e3e175e-0809-4277-ae78-e0d1156fcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a method that generates 10 csv files each of which has two numbers on each line. The files have an ascending name. The files are saved in the directory preprocessed_data\n",
    "import random\n",
    "def generate_data():\n",
    "    for i in range(0,num_files):\n",
    "        with open(f'data/file{i}.csv', 'w') as f:\n",
    "                f.write(f'{random.randint(1,100)}\\n{random.randint(1,100)}')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "95c1cdde-0f23-43d3-a1d3-dac8956dfe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6094622d-8e27-410b-94f6-a2cae267038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix = \"sagemaker/spark-preprocess-demo/input/raw/data\".format(timestamp_prefix)\n",
    "output_prefix = \"sagemaker/spark-preprocess-demo/output\".format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5baf8e3e-13c3-4a6b-879a-d754300bb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a method that takes all the files from the data local folder and saves them in the S3 bucket with name bucket under the prefixe\n",
    "def upload_data():\n",
    "    s3_client = boto3.client('s3')\n",
    "    for i in range(0,num_files):\n",
    "        s3_client.upload_file(f'data/file{i}.csv', bucket, f'{input_prefix}/file{i}.csv')\n",
    "    return \"s3://{}/{}/\".format(bucket, input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "92de3b9d-68ea-4742-b0f0-b7c48ee00afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477886989750/sagemaker/spark-preprocess-demo/input/raw/data/\n",
      "s3://sagemaker-us-east-1-477886989750/sagemaker/spark-preprocess-demo/output\n"
     ]
    }
   ],
   "source": [
    "data_s3_uri=upload_data()\n",
    "print (data_s3_uri)\n",
    "s3_output_uri= f\"s3://{bucket}/{output_prefix}\"\n",
    "print(s3_output_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f30b25-1216-4002-82f2-0295899138e9",
   "metadata": {},
   "source": [
    "## 3. Write the Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6cf5577f-5618-4d90-8db4-d2d67165eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘code’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1ac745fd-1e81-4e9a-8b6a-b6847183309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def transform(spark, s3_input_data,s3_output_train_data):\n",
    "    print('Processing {} => {}'.format(s3_input_data, s3_output_train_data))\n",
    "\n",
    "    rdd = spark.sparkContext.wholeTextFiles(s3_input_data, 9)\n",
    "    sum_rdd = rdd.map(lambda x: sum(int(y) for y in x[1].split(\"\\n\")))\n",
    "\n",
    "    row = Row(\"sum\")\n",
    "    df = sum_rdd.map(row).toDF()  \n",
    "    df.show()\n",
    "\n",
    "    print('Saving to output file {}'.format(s3_output_train_data))\n",
    "    df.write.format('csv').option('header','true').save(f'{s3_output_train_data}/output.csv',mode='overwrite')\n",
    "\n",
    "    print('Wrote to output file:  {}'.format(s3_output_train_data))\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"pyspark-demo\").getOrCreate()\n",
    "\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    args = dict(zip(args_iter, args_iter))\n",
    "    print(args.keys())\n",
    "    # Retrieve the args and replace 's3://' with 's3a://'\n",
    "    s3_input_data = args['s3_input_data'].replace('s3://', 's3a://')\n",
    "    print(s3_input_data)\n",
    "\n",
    "    s3_output_data = args['s3_output_data'].replace('s3://', 's3a://')\n",
    "    print(s3_output_data)\n",
    "    \n",
    "    transform(spark,s3_input_data, s3_output_data)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7fe6f-53c3-457f-8c5d-3ced75f33512",
   "metadata": {},
   "source": [
    "## 4. Run the Processing with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f1bee017-a851-4233-a183-5409a41e7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-2024-01-22-16-50-32-361\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=num_instances, #pyspark_process_instance_count\n",
    "    instance_type=\"ml.m5.4xlarge\", #pyspark_process_instance_type\n",
    "    max_runtime_in_seconds=2400\n",
    ")\n",
    "\n",
    "\n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\":{\n",
    "            \"spark.executor.memory\":\"10g\",\n",
    "            \"spark.executor.memoryOverhead\":\"5g\",\n",
    "            \"spark.driver.memory\":\"10g\",\n",
    "            \"spark.driver.memoryOverhead\":\"10g\",\n",
    "            \"spark.driver.maxResultSize\":\"10g\",\n",
    "            \"spark.executor.cores\":num_spark_cores,\n",
    "            \"spark.executor.instances\":num_spark_instances,\n",
    "            \"spark.yarn.maxAppAttempts\":1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/preprocess.py\", #pyspark_process_code\n",
    "    spark_event_logs_s3_uri=\"s3://{}/sagemaker/spark-preprocess-demo/spark_event_logs\".format(bucket),\n",
    "    arguments=[\n",
    "        \"s3_input_data\", data_s3_uri,\n",
    "        \"s3_output_data\", s3_output_uri \n",
    "    ],\n",
    "    #configuration=configuration, #optional. the default configuration will create a single executor per instance, allocating all memory and cores to this executor\n",
    "    outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='process-job',\n",
    "                                        source='/opt/ml/processing/output')\n",
    "              ],          \n",
    "    logs=True,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "48df2df6-eef1-4f0e-bfed-2e4a3ab990cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processing_job_name = spark_processor.jobs[-1].describe()['ProcessingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e1df978-a89a-4aef-b85a-c41c9676637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2366/2061528379.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sm-spark-2024-01-22-16-50-32-361\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, spark_processing_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "70eb228c-bdd6-40fc-8c12-8b78867ca7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2366/993684544.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sm-spark-2024-01-22-16-50-32-361;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ecc21724-2e90-49a1-85ae-1abdf8f8100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2366/3074493321.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-477886989750/?region=us-east-1&bucketType=general&prefix=sagemaker/spark-preprocess-demo/output\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/?region={}&bucketType=general&prefix={}\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket,region, output_prefix)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60290d-87ae-4c88-b9a6-38f9fe2dadcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53197d-d388-445e-924d-3595f506e05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
