{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dfb474-616d-41f8-a46a-9f5d345f6be9",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda7f5ef-b88e-4e00-9ede-621ac5ef36f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-477886989750\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433f5edd-b6fd-4317-a208-6226e20461d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files=10\n",
    "num_instances=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf45b3-6502-4519-9733-051659612a7c",
   "metadata": {},
   "source": [
    "# 2. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870dcf33-a2b2-40e3-9a99-62360869482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3e175e-0809-4277-ae78-e0d1156fcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a method that generates 10 csv files each of which has two numbers on each line. The files have an ascending name. The files are saved in the directory preprocessed_data\n",
    "import random\n",
    "def generate_data():\n",
    "    for i in range(0,num_files):\n",
    "        with open(f'data/file{i}.csv', 'w') as f:\n",
    "            f.write('income\\n')\n",
    "            for j in range(0,2):\n",
    "                f.write(f'{random.randint(1,100)}\\n')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c1cdde-0f23-43d3-a1d3-dac8956dfe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6094622d-8e27-410b-94f6-a2cae267038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker/spark-preprocess-demo/2024-01-20-13-17-41/input/raw/data\n"
     ]
    }
   ],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix = \"{}/input/raw/data\".format(prefix)\n",
    "input_preprocessed_prefix = \"{}/input/preprocessed/data\".format(prefix)\n",
    "print(input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5baf8e3e-13c3-4a6b-879a-d754300bb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a method that takes all the files from the data local folder and saves them in the S3 bucket with name bucket under the prefixe\n",
    "def upload_data():\n",
    "    s3_client = boto3.client('s3')\n",
    "    for i in range(0,10):\n",
    "        s3_client.upload_file(f'data/file{i}.csv', bucket, f'{input_prefix}/file{i}.csv')\n",
    "    return \"s3://{}/{}/\".format(bucket, input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92de3b9d-68ea-4742-b0f0-b7c48ee00afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477886989750/sagemaker/spark-preprocess-demo/2024-01-20-13-04-36/input/raw/data/\n"
     ]
    }
   ],
   "source": [
    "data_s3_uri=upload_data()\n",
    "print (data_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f30b25-1216-4002-82f2-0295899138e9",
   "metadata": {},
   "source": [
    "## 3. Write the Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf5577f-5618-4d90-8db4-d2d67165eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac745fd-1e81-4e9a-8b6a-b6847183309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_input_bucket\", type=str, help=\"s3 input bucket\")\n",
    "    parser.add_argument(\"--s3_input_key_prefix\", type=str, help=\"s3 input key prefix\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"CSVSum\").getOrCreate()\n",
    "\n",
    "    customSchema = StructType([\n",
    "        StructField(\"income\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    df = spark.read.format(\"csv\").schema(customSchema).load(\"s3://\" + os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)).select(\"*\", \"_metadata.file_name\")\n",
    "\n",
    "    sum_df = df.groupBy('file_name').agg(_sum('income').alias('income_sum'))\n",
    "    processed_rdd = sum_df.rdd\n",
    "    processed_rdd.coalesce(1).saveAsTextFile(\"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"processed\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7fe6f-53c3-457f-8c5d-3ced75f33512",
   "metadata": {},
   "source": [
    "## 4. Run the Processing with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bee017-a851-4233-a183-5409a41e7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the raw input dataset to a unique S3 location\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.executor.memory\": \"2g\", \"spark.executor.cores\": \"1\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/preprocess.py\",\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_input_key_prefix\",\n",
    "        input_prefix,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        input_preprocessed_prefix,\n",
    "    ],\n",
    "    configuration=configuration,\n",
    "    logs=False\n",
    ")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
